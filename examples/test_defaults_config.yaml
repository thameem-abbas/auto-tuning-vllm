# Test configuration that uses vLLM defaults instead of hardcoded values
study:
  name: "vllm_defaults_test"
  database_url: "postgresql://user:pass@localhost/optuna"
  
optimization:
  objective: "maximize"
  sampler: "tpe"
  n_trials: 50
  
benchmark:
  benchmark_type: "guidellm"
  model: "facebook/opt-125m"
  max_seconds: 300
  dataset: null
  prompt_tokens: 1000
  output_tokens: 500
  
logging:
  file_path: "/tmp/auto-tune-vllm-logs"
  log_level: "INFO"
  
parameters:
  # This parameter will use schema defaults since we don't specify ranges
  max_num_batched_tokens:
    enabled: true
    # No min/max specified - should use schema defaults
    
  # This parameter should use vLLM CLI defaults for gpu_memory_utilization (0.9)
  gpu_memory_utilization:
    enabled: true
    # Will fall back to vLLM default of 0.9, then allow optimization around that
    min: 0.8
    max: 0.95
    step: 0.01
    
  # Test parameter that uses vLLM defaults
  kv_cache_dtype:
    enabled: true
    # Should use vLLM CLI default options
    
  # Test boolean parameter with vLLM defaults
  enable_cuda_graphs:
    enabled: true
    # Should use vLLM CLI default behavior
